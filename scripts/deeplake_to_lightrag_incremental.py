#!/usr/bin/env python3
"""
DeepLake to LightRAG Incremental Ingestion Pipeline
====================================================
FIXED VERSION: Properly tracks document IDs to skip already-processed documents
and only reprocess modified content.

Key improvements:
1. Uses table identifiers as document IDs
2. LightRAG automatically skips duplicates via doc_status storage
3. Supports incremental updates without full reprocessing

Author: HybridRAG System
Date: 2025-10-04
"""

import os
import json
import asyncio
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import deeplake
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status
from dotenv import load_dotenv
import time
from tqdm import tqdm
from datetime import datetime, timedelta

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

class DeepLakeToLightRAGIncremental:
    """
    Incremental ingestion from DeepLake to LightRAG with proper duplicate detection.
    """

    def __init__(
        self,
        deeplake_path: str = "/media/gyasis/Drive 2/Deeplake_Storage/athena_descriptions_v4",
        lightrag_working_dir: str = "./athena_lightrag_db",
        openai_api_key: Optional[str] = None
    ):
        """
        Initialize the incremental converter.

        Args:
            deeplake_path: Path to the DeepLake database
            lightrag_working_dir: Working directory for LightRAG database
            openai_api_key: OpenAI API key (if not in env)
        """
        self.deeplake_path = deeplake_path
        self.lightrag_working_dir = lightrag_working_dir
        # Prefer Azure API key, fall back to OpenAI
        self.api_key = openai_api_key or os.getenv("AZURE_API_KEY") or os.getenv("OPENAI_API_KEY")

        if not self.api_key:
            raise ValueError("API key not found. Set AZURE_API_KEY or OPENAI_API_KEY environment variable.")

        # Create working directory
        Path(self.lightrag_working_dir).mkdir(parents=True, exist_ok=True)

        # Initialize LightRAG
        self._init_lightrag()

        # Open DeepLake dataset
        self._open_deeplake()

    def _init_lightrag(self):
        """Initialize LightRAG with Azure models (via LiteLLM)."""
        logger.info(f"Initializing LightRAG in {self.lightrag_working_dir}")

        # Get model names from environment (Azure preferred)
        llm_model = os.getenv("LIGHTRAG_MODEL", "azure/gpt-5.1")
        embed_model = os.getenv("LIGHTRAG_EMBED_MODEL", "azure/text-embedding-3-small")

        self.rag = LightRAG(
            working_dir=self.lightrag_working_dir,
            llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs:
                openai_complete_if_cache(
                    llm_model,
                    prompt,
                    system_prompt=system_prompt,
                    history_messages=history_messages,
                    api_key=self.api_key,
                    **kwargs
                ),
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                func=lambda texts: openai_embed(
                    texts,
                    model=embed_model,
                    api_key=self.api_key
                ),
            ),
            max_parallel_insert=2,  # Process 2 documents at a time
            llm_model_max_async=2,  # Reduced from 4 to minimize rate limiting
        )
        logger.info(f"LightRAG initialized successfully (LLM: {llm_model}, Embed: {embed_model})")

    async def _initialize_lightrag_storages(self):
        """Initialize LightRAG storages asynchronously."""
        logger.info("Initializing LightRAG storages...")
        await self.rag.initialize_storages()
        await initialize_pipeline_status()
        logger.info("LightRAG storages initialized successfully")

    def _open_deeplake(self):
        """Open the DeepLake dataset."""
        logger.info(f"Opening DeepLake dataset at {self.deeplake_path}")
        try:
            self.ds = deeplake.open(self.deeplake_path)
            logger.info(f"DeepLake dataset opened. Length: {len(self.ds)}")
        except Exception as e:
            logger.error(f"Failed to open DeepLake dataset: {e}")
            raise

    def parse_jsonl_text(self, text: str) -> Dict:
        """Parse JSONL text from DeepLake."""
        try:
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON: {e}")
            return {}

    def generate_document_id(self, record: Dict) -> str:
        """
        Generate a unique, stable document ID from record data.

        Uses SCHEMA.TABLE pattern for consistency.
        """
        table_name = str(record.get("TABLE NAME", "Unknown"))
        schema_name = str(record.get("SCHEMANAME", "Unknown"))

        # Create stable ID: "athena.{schema}.{table}"
        doc_id = f"athena.{schema_name}.{table_name}"
        return doc_id

    def format_document(self, record: Dict, metadata) -> str:
        """Format a DeepLake record into a readable document."""
        doc_parts = []

        # Title
        table_name = str(record.get("TABLE NAME", "Unknown Table"))
        schema_name = str(record.get("SCHEMANAME", "Unknown Schema"))
        doc_parts.append(f"# {schema_name}.{table_name}")
        doc_parts.append("")

        # Description
        if "TABLE DESCRIPTION" in record:
            doc_parts.append(f"## Description")
            doc_parts.append(str(record["TABLE DESCRIPTION"]))
            doc_parts.append("")

        # Details
        if "COMMENTS" in record:
            doc_parts.append(f"## Details")
            doc_parts.append(str(record["COMMENTS"]))
            doc_parts.append("")

        # Metadata
        doc_parts.append(f"## Metadata")
        if "TABLEID" in record:
            doc_parts.append(f"- **Table ID**: {str(record['TABLEID'])}")
        if "CATEGORY" in record:
            doc_parts.append(f"- **Category**: {str(record['CATEGORY'])}")
        if "RELEASE VERSION" in record:
            doc_parts.append(f"- **Release Version**: {str(record['RELEASE VERSION'])}")
        if "RELEASE DATE" in record:
            doc_parts.append(f"- **Release Date**: {str(record['RELEASE DATE'])}")
        if "ROW COUNT" in record:
            doc_parts.append(f"- **Row Count**: {str(record['ROW COUNT'])}")

        doc_parts.append("")
        doc_parts.append(f"---")
        doc_parts.append(f"*Source: DeepLake athena_descriptions_v4*")

        return "\n".join(doc_parts)

    def extract_documents_with_ids(self, batch_size: int = 100) -> Tuple[List[str], List[str]]:
        """
        Extract documents AND their IDs from DeepLake.

        Returns:
            Tuple of (documents, document_ids)
        """
        documents = []
        document_ids = []
        total_records = len(self.ds)
        processed_count = 0
        error_count = 0

        print(f"\nðŸ” EXTRACTION PHASE: Processing {total_records:,} records from DeepLake")
        print("=" * 70)

        with tqdm(total=total_records, desc="Extracting documents", unit="docs") as pbar:
            for i in range(0, total_records, batch_size):
                batch_end = min(i + batch_size, total_records)

                try:
                    texts = self.ds["text"][i:batch_end]
                    metadatas = self.ds["metadata"][i:batch_end]
                except Exception as e:
                    logger.error(f"Failed to access batch {i}-{batch_end}: {e}")
                    pbar.update(batch_end - i)
                    error_count += batch_end - i
                    continue

                for j, (text, metadata) in enumerate(zip(texts, metadatas)):
                    try:
                        record = self.parse_jsonl_text(text)

                        if record and "TABLE NAME" in record and "SCHEMANAME" in record:
                            # Generate stable ID
                            doc_id = self.generate_document_id(record)

                            # Format document
                            document = self.format_document(record, metadata)

                            documents.append(document)
                            document_ids.append(doc_id)
                            processed_count += 1
                        else:
                            logger.warning(f"Missing essential fields at index {i+j}")
                            error_count += 1

                    except Exception as e:
                        logger.error(f"Error processing record {i+j}: {e}")
                        error_count += 1
                        continue

                    pbar.update(1)

                success_rate = (processed_count / (processed_count + error_count)) * 100 if (processed_count + error_count) > 0 else 0
                pbar.set_postfix({
                    'Success': f"{processed_count:,}",
                    'Errors': f"{error_count:,}",
                    'Rate': f"{success_rate:.1f}%"
                })

        print(f"\nâœ… EXTRACTION COMPLETE:")
        print(f"   â€¢ Successfully processed: {processed_count:,} documents")
        print(f"   â€¢ Errors encountered: {error_count:,} records")
        print(f"   â€¢ Success rate: {(processed_count/(processed_count+error_count)*100):.1f}%")
        print(f"   â€¢ Total extracted: {len(documents):,} documents with IDs")

        return documents, document_ids

    async def ingest_to_lightrag_incremental(
        self,
        documents: List[str],
        document_ids: List[str],
        batch_size: int = 10
    ):
        """
        Ingest documents with IDs - LightRAG will skip duplicates automatically.

        Args:
            documents: List of formatted documents
            document_ids: List of document IDs (must match documents length)
            batch_size: Number of documents to insert at once
        """
        if len(documents) != len(document_ids):
            raise ValueError(f"Documents ({len(documents)}) and IDs ({len(document_ids)}) count mismatch")

        total_docs = len(documents)
        ingested_count = 0
        skipped_count = 0
        failed_count = 0
        start_time = time.time()

        print(f"\nðŸ“š INCREMENTAL INGESTION: Loading {total_docs:,} documents into LightRAG")
        print("=" * 70)
        print(f"ðŸ“Š Configuration:")
        print(f"   â€¢ Batch size: {batch_size} documents")
        print(f"   â€¢ Expected batches: {(total_docs + batch_size - 1) // batch_size}")
        print(f"   â€¢ Duplicate detection: Enabled via document IDs")
        print()

        with tqdm(total=total_docs, desc="Ingesting to LightRAG", unit="docs") as pbar:
            for i in range(0, total_docs, batch_size):
                batch_docs = documents[i:i+batch_size]
                batch_ids = document_ids[i:i+batch_size]
                batch_end = min(i + batch_size, total_docs)

                try:
                    # LightRAG will filter out already-processed IDs internally
                    await self.rag.ainsert(batch_docs, ids=batch_ids)

                    # All in batch considered processed (LightRAG handles duplicates internally)
                    batch_processed = len(batch_docs)
                    ingested_count += batch_processed

                    pbar.update(len(batch_docs))

                    # Calculate stats
                    elapsed = time.time() - start_time
                    if ingested_count > 0:
                        rate = ingested_count / elapsed
                        remaining = total_docs - ingested_count
                        eta_seconds = remaining / rate if rate > 0 else 0
                        eta = timedelta(seconds=int(eta_seconds))

                        pbar.set_postfix({
                            'Processed': f"{ingested_count:,}",
                            'Failed': f"{failed_count:,}",
                            'Rate': f"{rate:.1f}/s",
                            'ETA': str(eta)
                        })

                    # Rate limiting - increased to reduce API rate limit errors
                    delay = 5.0  # Fixed 5-second delay between batches
                    await asyncio.sleep(delay)

                except Exception as e:
                    logger.error(f"Batch {i}-{batch_end} failed: {e}")

                    # Try individual insertion with longer delays
                    for j, (doc, doc_id) in enumerate(zip(batch_docs, batch_ids)):
                        try:
                            await self.rag.ainsert([doc], ids=[doc_id])
                            ingested_count += 1
                            await asyncio.sleep(2.0)  # Longer delay for individual retries
                        except Exception as e2:
                            logger.error(f"Document {doc_id} failed: {str(e2)[:100]}...")
                            failed_count += 1

                        pbar.update(1)

        # Final statistics
        total_time = time.time() - start_time
        success_rate = (ingested_count / total_docs) * 100 if total_docs > 0 else 0
        avg_rate = ingested_count / total_time if total_time > 0 else 0

        print(f"\nâœ… INCREMENTAL INGESTION COMPLETE:")
        print(f"   â€¢ Processed: {ingested_count:,} documents")
        print(f"   â€¢ Failed: {failed_count:,} documents")
        print(f"   â€¢ Success rate: {success_rate:.1f}%")
        print(f"   â€¢ Total time: {timedelta(seconds=int(total_time))}")
        print(f"   â€¢ Average rate: {avg_rate:.1f} docs/second")
        print(f"\nðŸ’¡ Note: LightRAG automatically skips already-processed documents via ID tracking")

        logger.info(f"Incremental ingestion: {ingested_count}/{total_docs} documents ({success_rate:.1f}% success)")

    async def run_incremental_ingestion(self):
        """Run incremental ingestion pipeline."""
        pipeline_start_time = time.time()

        print(f"\nðŸš€ INCREMENTAL DEEPLAKEâ†’LIGHTRAG PIPELINE")
        print(f"{'='*70}")
        print(f"ðŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ðŸ“‚ Source: {self.deeplake_path}")
        print(f"ðŸ“ Target: {self.lightrag_working_dir}")
        llm_model = os.getenv("LIGHTRAG_MODEL", "azure/gpt-5.1")
        embed_model = os.getenv("LIGHTRAG_EMBED_MODEL", "azure/text-embedding-3-small")
        print(f"ðŸ¤– Model: {llm_model} ({embed_model})")
        print(f"âœ¨ Mode: INCREMENTAL (skips duplicates via document IDs)")
        print(f"{'='*70}")

        try:
            # Phase 1: Initialize
            print(f"\nâš™ï¸  INITIALIZATION PHASE")
            init_start = time.time()
            await self._initialize_lightrag_storages()
            init_time = time.time() - init_start
            print(f"âœ… Initialization: {init_time:.2f}s")

            # Phase 2: Extract with IDs
            extract_start = time.time()
            documents, document_ids = self.extract_documents_with_ids(batch_size=100)
            extract_time = time.time() - extract_start

            if not documents:
                print(f"\nâŒ PIPELINE FAILED: No documents extracted")
                return False

            print(f"\nâ±ï¸  Extraction: {timedelta(seconds=int(extract_time))}")

            # Phase 3: Incremental ingestion
            ingest_start = time.time()
            await self.ingest_to_lightrag_incremental(
                documents,
                document_ids,
                batch_size=8
            )
            ingest_time = time.time() - ingest_start

            print(f"\nâ±ï¸  Ingestion: {timedelta(seconds=int(ingest_time))}")

            # Phase 4: Validation
            print(f"\nðŸ” VALIDATION PHASE")
            await self._validate_ingestion()

            # Final summary
            total_time = time.time() - pipeline_start_time
            self._print_pipeline_summary(total_time, len(documents), init_time, extract_time, ingest_time)

            return True

        except Exception as e:
            total_time = time.time() - pipeline_start_time
            print(f"\nâŒ PIPELINE FAILED after {timedelta(seconds=int(total_time))}")
            print(f"   Error: {str(e)}")
            logger.error(f"Pipeline failed: {e}")
            return False

    async def _validate_ingestion(self):
        """Validate ingestion."""
        try:
            test_query = "What tables are available?"
            query_param = QueryParam(mode="naive")
            result = await self.rag.aquery(test_query, param=query_param)

            if result:
                print(f"âœ… Validation successful - LightRAG is responsive")
            else:
                print(f"âš ï¸  Validation warning - Empty response")

        except Exception as e:
            print(f"âŒ Validation failed: {str(e)}")

    def _print_pipeline_summary(self, total_time: float, doc_count: int, init_time: float, extract_time: float, ingest_time: float):
        """Print pipeline summary."""
        print(f"\nðŸŽ‰ INCREMENTAL PIPELINE COMPLETED!")
        print(f"{'='*70}")
        print(f"ðŸ“Š SUMMARY:")
        print(f"   â€¢ Documents processed: {doc_count:,}")
        print(f"   â€¢ Total time: {timedelta(seconds=int(total_time))}")
        print(f"   â€¢ Rate: {doc_count/total_time:.1f} docs/sec")
        print(f"\nâ±ï¸  BREAKDOWN:")
        print(f"   â€¢ Init: {init_time:.2f}s ({init_time/total_time*100:.1f}%)")
        print(f"   â€¢ Extract: {timedelta(seconds=int(extract_time))} ({extract_time/total_time*100:.1f}%)")
        print(f"   â€¢ Ingest: {timedelta(seconds=int(ingest_time))} ({ingest_time/total_time*100:.1f}%)")
        print(f"\nðŸŽ¯ NEXT STEPS:")
        print(f"   â€¢ Run again to verify duplicate skipping works!")
        print(f"   â€¢ Query: uv run python lightrag_query_demo.py")
        print(f"{'='*70}\n")


async def main():
    """Main function."""

    converter = DeepLakeToLightRAGIncremental(
        deeplake_path="/media/gyasis/Drive 2/Deeplake_Storage/athena_descriptions_v4",
        lightrag_working_dir="./athena_lightrag_db"
    )

    # Run incremental ingestion
    await converter.run_incremental_ingestion()

    # Test queries
    test_queries = [
        "What tables are related to appointments?",
        "Tell me about anesthesia case tables"
    ]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"Query: {query}")
        print(f"{'='*60}")

        result = await converter.rag.aquery(query, param=QueryParam(mode="hybrid"))
        if result:
            print(result[:500] + "..." if len(result) > 500 else result)


if __name__ == "__main__":
    asyncio.run(main())
